{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 접근법 최적화를 위한 주요 개념 이해\n",
    "    1. 종속 라이브러리 가져오기\n",
    "        - 케라스 라이브러리 사용\n",
    "    2. IMDb 데이터셋 내려받기 및 적재하기\n",
    "        - IMDb 데이터셋을 내려받고 케라스 API를 사용하여 데이터 적재\n",
    "    3. 흔한 단어 및 최대 텍스트 길이 선택\n",
    "        - 워드 임베딩 단계에서 사용할 수 있는 어휘의 값을 설정\n",
    "    4. 워드 임베딩 구현\n",
    "        - 케라스의 기본 포함 기술을 사용하고 길이가 300인 단어 벡터를 생성\n",
    "    5. CNN 구축\n",
    "        - 첫 번째 계층에 64개 뉴런이 있고 두 번째 계츠에 32개 뉴런, 마지막 계층에 16개 뉴런이 있는 3계층 신경망 생성\n",
    "        - 시그모이드 활성화 함수 사용\n",
    "    6. 훈련 및 정확도 획득\n",
    "        - 모델을 훈련하고 정확도 점수 생성\n",
    "        - 에포크 값을 3으로 설정하고 Adam을 최적화 함수로 설정, 손실 함수로 binary_crossentropy를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개선 접근법 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Import dependencies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Load datset and Load top 10000 words from idbm dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_words` argument in `load_data` has been renamed `num_words`.\n"
     ]
    }
   ],
   "source": [
    "# Using keras to load the datset with the top_words\n",
    "top_words = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set the maximum movie review length***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequence to the same length\n",
    "max_review_length = 1600\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Default Word embedding based on word2vec***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using embedding from Keras\n",
    "embedding_vecor_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Build Convolution Neural Net (CNN)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convoluetion model (3x conv, flatten, 2x dense)\n",
    "model.add(Convolution1D(64, 3, padding='same'))\n",
    "model.add(Convolution1D(32, 3, padding='same'))\n",
    "model.add(Convolution1D(16, 3, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(180, activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Train on Training dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  1/391 [..............................] - ETA: 0s - loss: 0.6873 - accuracy: 0.5781WARNING:tensorflow:From C:\\Users\\wltjd\\anaconda3\\envs\\ml_sol\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.5746 - accuracy: 0.6482\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 55s 141ms/step - loss: 0.2480 - accuracy: 0.9027\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 53s 136ms/step - loss: 0.1425 - accuracy: 0.9486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dabf935dc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log to tensorboard\n",
    "tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, callbacks=[tensorBoardCallback], batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Obtain training accuracy score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.94%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the train set\n",
    "scores = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Accuracy: {:.2f}%'.format(scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Obtain testing accuracy score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.58%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: {:.2f}%'.format(scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개선 접근법의 문제점 이해\n",
    "    - 사전 훈련 Word2Vec 또는 GloVe(global word vector, 전역 단어 모델) 모델을 사용해 단어 벡터를 생성할 수 있다\n",
    "    - LSTM을 사용하는 반복적인 신경망을 사용해 더 나은 결과를 얻어야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
